{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c74e806-cb10-431e-9d61-41109836646f",
   "metadata": {},
   "source": [
    "This notebook is to get the activations in embedding space over all possible mutations in TP53."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d292058b-eb0d-4130-9982-db36ec7df411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c3c7b3-5f25-46e4-9a6e-7882df2c6ccf",
   "metadata": {},
   "source": [
    "ESM-1b has 33 layers & 1280 embedding dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5d9f005-0a3c-4a18-b4f5-ea0980e2ef69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "batch_converter = alphabet.get_batch_converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6e4ba50-01bb-41c6-bfce-5348f52b4b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "amino_acids = [\n",
    "    \"A\",  # Alanine\n",
    "    \"R\",  # Arginine\n",
    "    \"N\",  # Asparagine\n",
    "    \"D\",  # Aspartic acid\n",
    "    \"C\",  # Cysteine\n",
    "    \"Q\",  # Glutamine\n",
    "    \"E\",  # Glutamic acid\n",
    "    \"G\",  # Glycine\n",
    "    \"H\",  # Histidine\n",
    "    \"I\",  # Isoleucine\n",
    "    \"L\",  # Leucine\n",
    "    \"K\",  # Lysine\n",
    "    \"M\",  # Methionine\n",
    "    \"F\",  # Phenylalanine\n",
    "    \"P\",  # Proline\n",
    "    \"S\",  # Serine\n",
    "    \"T\",  # Threonine\n",
    "    \"W\",  # Tryptophan\n",
    "    \"Y\",  # Tyrosine\n",
    "    \"V\"   # Valine\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0261435f-fd6c-41b5-a047-33ed400e8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tp53_sequence = \"MEEPQSDPSVEPPLSQETFSDLWKLLPENNVLSPLPSQAMDDLMLSPDDIEQWFTEDPGPDEAPRMPEAAPPVAPAPAAPTPAAPAPAPSWPLSSSVPSQKTYQGSYGFRLGFLHSGTAKSVTCTYSPALNKMFCQLAKTCPVQLWVDSTPPPGTRVRAMAIYKQSQHMTEVVRRCPHHERCSDSDGLAPPQHLIRVEGNLRVEYLDDRNTFRHSVVVPYEPPEVGSDCTTIHYNYMCNSSCMGGMNRRPILTIITLEDSSGNLLGRNSFEVRVCACPGRDRRTEEENLRKKGEPHHELPPGSTKRALPNNTSSSPQPKKKPLDGEYFTLQIRGRERFEMFRELNEALELKDAQAGKEPGGSRAHSSHLKSKKGQSTSRHKKLMFKTEGPDSD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "762758cb-a122-4def-8ac6-412ef51a9b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate_sequence(sequence, position, new_residue):\n",
    "    \"\"\"\n",
    "    Mutate the sequence at the given position (1-based indexing) to the new residue.\n",
    "\n",
    "    Args:\n",
    "        sequence (str): Original amino acid sequence.\n",
    "        position (int): 1-based index of the residue to mutate.\n",
    "        new_residue (str): New amino acid (single letter).\n",
    "\n",
    "    Returns:\n",
    "        str: Mutated sequence.\n",
    "    \"\"\"\n",
    "    if position < 1 or position > len(sequence):\n",
    "        raise ValueError(\"Position out of range.\")\n",
    "\n",
    "    if len(new_residue) != 1:\n",
    "        raise ValueError(\"New residue must be a single character.\")\n",
    "\n",
    "    mutated_sequence = sequence[:position - 1] + new_residue + sequence[position:]\n",
    "    return mutated_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "351b967f-6c41-4d8f-a410-1b06ee07bad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(data, layer = 33):\n",
    "    # Your protein sequence(s)\n",
    "    # data = [(\"wt\", tp53_sequence)]  # Replace with your sequence\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    \n",
    "    # Extract per-residue representations (on the last layer, layer 33)\n",
    "    with torch.no_grad():\n",
    "        results = model(batch_tokens, repr_layers=[layer], return_contacts=False)\n",
    "    \n",
    "    token_representations = results[\"representations\"][layer]\n",
    "    \n",
    "    # Remove padding ([CLS] and [EOS]) tokens to get per-residue embeddings\n",
    "    # Typically, token 0 is [CLS], token -1 is [EOS]\n",
    "    embedding = token_representations[0, 1:-1]  # Shape: (sequence_length, 1280)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ea07471-5886-49eb-b931-3e7ae39dc64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [(\"wt\", tp53_sequence)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba6a91ab-6efc-410d-8242-6db5c99bd176",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, len(tp53_sequence) + 1):\n",
    "    wt_aa = tp53_sequence[i - 1]\n",
    "    for aa in amino_acids:\n",
    "        if aa == wt_aa:\n",
    "            continue\n",
    "        data.append((f\"{wt_aa}{i}{aa}\", mutate_sequence(tp53_sequence, i, aa)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430e02b8-8841-41b9-8a27-473f2a337d4e",
   "metadata": {},
   "source": [
    "# run to get layer 33"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5add3a41-bdae-4613-85cc-25f5faad3173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESM Embeddings: 100%|█████████████████████████████████████████████████████████████████████████████████████████| 7468/7468 [1:20:59<00:00,  1.54it/s]\n"
     ]
    }
   ],
   "source": [
    "# batching doesn't really help with time so run one by one\n",
    "\n",
    "embedding_list = []\n",
    "\n",
    "for seq in tqdm(data, desc=\"Running ESM Embeddings\"):\n",
    "    embedding = get_embedding([seq])\n",
    "    embedding_list.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e31058f9-71e9-486f-8a3d-61f2b0e93b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7468"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "62435214-0dd9-4932-a145-608fc2eca9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# # Save to a file just in case\n",
    "# with open('embedding_list.pkl', 'wb') as f:\n",
    "#     pickle.dump(embedding_list, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4977ea83-4087-4e07-8b58-1d42de8da3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(zip([x[0] for x in data], embedding_list), columns = [\"name\", \"embedding\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "11e76bfb-04ee-4ce9-8242-4e48d9cbaf83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wt</td>\n",
       "      <td>[[tensor(0.1848), tensor(-0.0135), tensor(0.18...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1A</td>\n",
       "      <td>[[tensor(0.1435), tensor(-0.0347), tensor(0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M1R</td>\n",
       "      <td>[[tensor(0.2029), tensor(0.0074), tensor(0.206...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M1N</td>\n",
       "      <td>[[tensor(0.1319), tensor(0.0299), tensor(0.125...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M1D</td>\n",
       "      <td>[[tensor(0.1527), tensor(0.0441), tensor(0.073...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name                                          embedding\n",
       "0   wt  [[tensor(0.1848), tensor(-0.0135), tensor(0.18...\n",
       "1  M1A  [[tensor(0.1435), tensor(-0.0347), tensor(0.05...\n",
       "2  M1R  [[tensor(0.2029), tensor(0.0074), tensor(0.206...\n",
       "3  M1N  [[tensor(0.1319), tensor(0.0299), tensor(0.125...\n",
       "4  M1D  [[tensor(0.1527), tensor(0.0441), tensor(0.073..."
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6d49c0ad-4e99-4977-b2ce-29b82b2cafff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"all_mut_layer_33.tsv\", sep = \"\\t\", index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "64d1b91a-5a53-4857-9c9b-79fc48275bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = pd.read_csv(\"all_mut_layer_33.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a11fda5c-9de2-45b3-bdf2-456b05e58727",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_mut_layer_33.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "449116f4-fe6d-4b9d-bf5a-4cf1a55a861a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([393, 1280])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"embedding\"].iloc[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ce9887-65a2-49ee-a82b-e7f4ee3457ae",
   "metadata": {},
   "source": [
    "## get mean activation over different residue positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af9e866-0ece-4205-a6be-b8a8874a4629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['embedding_mean'] = df['embedding'].apply(lambda x: x.mean(dim=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "966d1732-7f09-440d-9ec5-a01b99e13ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_mut_layer_33_pos_mean.pkl', 'wb') as f:\n",
    "    pickle.dump(df[[\"name\", \"embedding_mean\"]], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "872c179c-27fa-481d-a5c1-9a8994b7a35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('all_mut_layer_33_pos_mean.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "19ad524b-9721-4d28-8957-4a774ea310f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0001)\n"
     ]
    }
   ],
   "source": [
    "## variance is super low\n",
    "\n",
    "tensor_lists = data['embedding_mean'].tolist()\n",
    "\n",
    "# Convert each list of tensors into a tensor of floats\n",
    "tensor_lists = [torch.stack([t for t in tensor_list]) for tensor_list in tensor_lists]\n",
    "\n",
    "# Stack all the tensors into one big 2D tensor (rows = samples, cols = embedding dimension)\n",
    "all_embeddings = torch.stack(tensor_lists)\n",
    "\n",
    "# Now compute variance across rows (dim=0)\n",
    "variance = torch.var(all_embeddings, dim=0, unbiased=True)  # unbiased=True matches pandas default\n",
    "\n",
    "print(variance.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670b23c3-5456-420e-b245-2d0bcf345a42",
   "metadata": {},
   "source": [
    "## get activation diff for the mutated position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3c264680-4701-429e-b0fb-350d97c33b53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>embedding</th>\n",
       "      <th>embedding_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wt</td>\n",
       "      <td>[[tensor(0.1848), tensor(-0.0135), tensor(0.18...</td>\n",
       "      <td>[tensor(-0.0231), tensor(0.1752), tensor(-0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1A</td>\n",
       "      <td>[[tensor(0.1435), tensor(-0.0347), tensor(0.05...</td>\n",
       "      <td>[tensor(-0.0299), tensor(0.1748), tensor(-0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M1R</td>\n",
       "      <td>[[tensor(0.2029), tensor(0.0074), tensor(0.206...</td>\n",
       "      <td>[tensor(-0.0287), tensor(0.1731), tensor(-0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M1N</td>\n",
       "      <td>[[tensor(0.1319), tensor(0.0299), tensor(0.125...</td>\n",
       "      <td>[tensor(-0.0255), tensor(0.1742), tensor(-0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M1D</td>\n",
       "      <td>[[tensor(0.1527), tensor(0.0441), tensor(0.073...</td>\n",
       "      <td>[tensor(-0.0298), tensor(0.1734), tensor(-0.02...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  name                                          embedding  \\\n",
       "0   wt  [[tensor(0.1848), tensor(-0.0135), tensor(0.18...   \n",
       "1  M1A  [[tensor(0.1435), tensor(-0.0347), tensor(0.05...   \n",
       "2  M1R  [[tensor(0.2029), tensor(0.0074), tensor(0.206...   \n",
       "3  M1N  [[tensor(0.1319), tensor(0.0299), tensor(0.125...   \n",
       "4  M1D  [[tensor(0.1527), tensor(0.0441), tensor(0.073...   \n",
       "\n",
       "                                      embedding_mean  \n",
       "0  [tensor(-0.0231), tensor(0.1752), tensor(-0.02...  \n",
       "1  [tensor(-0.0299), tensor(0.1748), tensor(-0.02...  \n",
       "2  [tensor(-0.0287), tensor(0.1731), tensor(-0.02...  \n",
       "3  [tensor(-0.0255), tensor(0.1742), tensor(-0.02...  \n",
       "4  [tensor(-0.0298), tensor(0.1734), tensor(-0.02...  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "590203d3-cc52-4350-a0e0-168189558050",
   "metadata": {},
   "outputs": [],
   "source": [
    "wild_type = df.iloc[0][\"embedding\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f96a549f-aace-4688-9d9d-2ce8f786b4fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([393, 1280])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wild_type.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "286f5dfc-2e26-4d5a-a368-de3d390718e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to apply\n",
    "def subtract_from_wild_type(row):\n",
    "    if row[\"name\"] == \"wt\":\n",
    "        return 0\n",
    "    index_0dim = int(row[\"name\"][1:-1])\n",
    "    diff = row['embedding'] - wild_type\n",
    "    # index\n",
    "    return diff[index_0dim-1]\n",
    "\n",
    "# apply to the dataframe\n",
    "df['embedding_diff'] = df.apply(subtract_from_wild_type, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ff3fedf7-a360-40df-a391-1ee206e51336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>embedding</th>\n",
       "      <th>embedding_mean</th>\n",
       "      <th>embedding_diff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wt</td>\n",
       "      <td>[[tensor(0.1848), tensor(-0.0135), tensor(0.18...</td>\n",
       "      <td>[tensor(-0.0231), tensor(0.1752), tensor(-0.02...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M1A</td>\n",
       "      <td>[[tensor(0.1435), tensor(-0.0347), tensor(0.05...</td>\n",
       "      <td>[tensor(-0.0299), tensor(0.1748), tensor(-0.02...</td>\n",
       "      <td>[tensor(-0.0413), tensor(-0.0212), tensor(-0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M1R</td>\n",
       "      <td>[[tensor(0.2029), tensor(0.0074), tensor(0.206...</td>\n",
       "      <td>[tensor(-0.0287), tensor(0.1731), tensor(-0.02...</td>\n",
       "      <td>[tensor(0.0181), tensor(0.0210), tensor(0.0262...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M1N</td>\n",
       "      <td>[[tensor(0.1319), tensor(0.0299), tensor(0.125...</td>\n",
       "      <td>[tensor(-0.0255), tensor(0.1742), tensor(-0.02...</td>\n",
       "      <td>[tensor(-0.0529), tensor(0.0434), tensor(-0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M1D</td>\n",
       "      <td>[[tensor(0.1527), tensor(0.0441), tensor(0.073...</td>\n",
       "      <td>[tensor(-0.0298), tensor(0.1734), tensor(-0.02...</td>\n",
       "      <td>[tensor(-0.0320), tensor(0.0577), tensor(-0.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7463</th>\n",
       "      <td>D393S</td>\n",
       "      <td>[[tensor(0.1874), tensor(-0.0092), tensor(0.18...</td>\n",
       "      <td>[tensor(-0.0249), tensor(0.1759), tensor(-0.02...</td>\n",
       "      <td>[tensor(-0.2499), tensor(-0.0110), tensor(0.03...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7464</th>\n",
       "      <td>D393T</td>\n",
       "      <td>[[tensor(0.1870), tensor(-0.0067), tensor(0.18...</td>\n",
       "      <td>[tensor(-0.0231), tensor(0.1754), tensor(-0.02...</td>\n",
       "      <td>[tensor(-0.0735), tensor(0.0936), tensor(-0.06...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7465</th>\n",
       "      <td>D393W</td>\n",
       "      <td>[[tensor(0.1946), tensor(-0.0108), tensor(0.18...</td>\n",
       "      <td>[tensor(-0.0224), tensor(0.1722), tensor(-0.02...</td>\n",
       "      <td>[tensor(-0.0679), tensor(-0.1721), tensor(-0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7466</th>\n",
       "      <td>D393Y</td>\n",
       "      <td>[[tensor(0.1956), tensor(-0.0135), tensor(0.17...</td>\n",
       "      <td>[tensor(-0.0220), tensor(0.1710), tensor(-0.02...</td>\n",
       "      <td>[tensor(-0.0593), tensor(-0.1182), tensor(-0.2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7467</th>\n",
       "      <td>D393V</td>\n",
       "      <td>[[tensor(0.1939), tensor(-0.0056), tensor(0.18...</td>\n",
       "      <td>[tensor(-0.0222), tensor(0.1749), tensor(-0.02...</td>\n",
       "      <td>[tensor(-0.0046), tensor(0.0367), tensor(-0.11...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7468 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       name                                          embedding  \\\n",
       "0        wt  [[tensor(0.1848), tensor(-0.0135), tensor(0.18...   \n",
       "1       M1A  [[tensor(0.1435), tensor(-0.0347), tensor(0.05...   \n",
       "2       M1R  [[tensor(0.2029), tensor(0.0074), tensor(0.206...   \n",
       "3       M1N  [[tensor(0.1319), tensor(0.0299), tensor(0.125...   \n",
       "4       M1D  [[tensor(0.1527), tensor(0.0441), tensor(0.073...   \n",
       "...     ...                                                ...   \n",
       "7463  D393S  [[tensor(0.1874), tensor(-0.0092), tensor(0.18...   \n",
       "7464  D393T  [[tensor(0.1870), tensor(-0.0067), tensor(0.18...   \n",
       "7465  D393W  [[tensor(0.1946), tensor(-0.0108), tensor(0.18...   \n",
       "7466  D393Y  [[tensor(0.1956), tensor(-0.0135), tensor(0.17...   \n",
       "7467  D393V  [[tensor(0.1939), tensor(-0.0056), tensor(0.18...   \n",
       "\n",
       "                                         embedding_mean  \\\n",
       "0     [tensor(-0.0231), tensor(0.1752), tensor(-0.02...   \n",
       "1     [tensor(-0.0299), tensor(0.1748), tensor(-0.02...   \n",
       "2     [tensor(-0.0287), tensor(0.1731), tensor(-0.02...   \n",
       "3     [tensor(-0.0255), tensor(0.1742), tensor(-0.02...   \n",
       "4     [tensor(-0.0298), tensor(0.1734), tensor(-0.02...   \n",
       "...                                                 ...   \n",
       "7463  [tensor(-0.0249), tensor(0.1759), tensor(-0.02...   \n",
       "7464  [tensor(-0.0231), tensor(0.1754), tensor(-0.02...   \n",
       "7465  [tensor(-0.0224), tensor(0.1722), tensor(-0.02...   \n",
       "7466  [tensor(-0.0220), tensor(0.1710), tensor(-0.02...   \n",
       "7467  [tensor(-0.0222), tensor(0.1749), tensor(-0.02...   \n",
       "\n",
       "                                         embedding_diff  \n",
       "0                                                     0  \n",
       "1     [tensor(-0.0413), tensor(-0.0212), tensor(-0.1...  \n",
       "2     [tensor(0.0181), tensor(0.0210), tensor(0.0262...  \n",
       "3     [tensor(-0.0529), tensor(0.0434), tensor(-0.05...  \n",
       "4     [tensor(-0.0320), tensor(0.0577), tensor(-0.10...  \n",
       "...                                                 ...  \n",
       "7463  [tensor(-0.2499), tensor(-0.0110), tensor(0.03...  \n",
       "7464  [tensor(-0.0735), tensor(0.0936), tensor(-0.06...  \n",
       "7465  [tensor(-0.0679), tensor(-0.1721), tensor(-0.0...  \n",
       "7466  [tensor(-0.0593), tensor(-0.1182), tensor(-0.2...  \n",
       "7467  [tensor(-0.0046), tensor(0.0367), tensor(-0.11...  \n",
       "\n",
       "[7468 rows x 4 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "1337dff0-e5be-4c9f-908b-a8ab0fbd910d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0046,  0.0367, -0.1130,  ..., -0.2177, -0.1049,  0.0407])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(df.iloc[7467][\"embedding\"] - wild_type)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "7ea35e45-60fc-439d-882c-23ae7fc095c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_mut_layer_33_diff.pkl', 'wb') as f:\n",
    "    pickle.dump(df[[\"name\", \"embedding_diff\"]], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a4e3a8-b94e-4046-8b83-7084c3e4aaa8",
   "metadata": {},
   "source": [
    "# get layer 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5cd8f49f-e2c5-425c-bd34-87ebb56a253b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESM Embeddings:   4%|███▎                                                                                       | 272/7468 [07:23<3:15:29,  1.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m embedding_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seq \u001b[38;5;129;01min\u001b[39;00m tqdm(data, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning ESM Embeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     embedding_list\u001b[38;5;241m.\u001b[39mappend(embedding)\n",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m, in \u001b[0;36mget_embedding\u001b[0;34m(data, layer)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Extract per-residue representations (on the last layer, layer 33)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepr_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_contacts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m token_representations \u001b[38;5;241m=\u001b[39m results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepresentations\u001b[39m\u001b[38;5;124m\"\u001b[39m][layer]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Remove padding ([CLS] and [EOS]) tokens to get per-residue embeddings\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Typically, token 0 is [CLS], token -1 is [EOS]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/esm/model/esm1.py:156\u001b[0m, in \u001b[0;36mProteinBertModel.forward\u001b[0;34m(self, tokens, repr_layers, need_head_weights, return_contacts)\u001b[0m\n\u001b[1;32m    153\u001b[0m     padding_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m--> 156\u001b[0m     x, attn \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_head_weights\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (layer_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m repr_layers:\n\u001b[1;32m    160\u001b[0m         hidden_representations[layer_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/esm/modules.py:125\u001b[0m, in \u001b[0;36mTransformerLayer.forward\u001b[0;34m(self, x, self_attn_mask, self_attn_padding_mask, need_head_weights)\u001b[0m\n\u001b[1;32m    123\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    124\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(x)\n\u001b[0;32m--> 125\u001b[0m x, attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mneed_head_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_head_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m x \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    136\u001b[0m residual \u001b[38;5;241m=\u001b[39m x\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/esm/multihead_attention.py:208\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, incremental_state, need_weights, static_kv, attn_mask, before_softmax, need_head_weights)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrot_emb\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menable_torch_version\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m need_head_weights\n\u001b[1;32m    206\u001b[0m ):\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_separate_proj_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mq_proj_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mk_proj_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mv_proj_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m incremental_state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m     saved_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_input_buffer(incremental_state)\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/functional.py:5429\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5427\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5428\u001b[0m         b_q, b_k, b_v \u001b[38;5;241m=\u001b[39m in_proj_bias\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m-> 5429\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m \u001b[43m_in_projection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_q\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_v\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5431\u001b[0m \u001b[38;5;66;03m# prep attention mask\u001b[39;00m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;66;03m# ensure attn_mask's dim is 3\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/py312/lib/python3.12/site-packages/torch/nn/functional.py:4996\u001b[0m, in \u001b[0;36m_in_projection\u001b[0;34m(q, k, v, w_q, w_k, w_v, b_q, b_k, b_v)\u001b[0m\n\u001b[1;32m   4994\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m b_k \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m b_k\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (Eq,), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting key bias shape of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(Eq,)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb_k\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4995\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m b_v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m b_v\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (Eq,), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpecting value bias shape of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(Eq,)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mb_v\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 4996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m linear(q, w_q, b_q), \u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_k\u001b[49m\u001b[43m)\u001b[49m, linear(v, w_v, b_v)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# batching doesn't really help with time so run one by one\n",
    "\n",
    "embedding_list = []\n",
    "\n",
    "for seq in tqdm(data, desc=\"Running ESM Embeddings\"):\n",
    "    embedding = get_embedding([seq], layer = 0)\n",
    "    embedding_list.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54176733-491d-45f1-bc00-ee1d4a0882dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
